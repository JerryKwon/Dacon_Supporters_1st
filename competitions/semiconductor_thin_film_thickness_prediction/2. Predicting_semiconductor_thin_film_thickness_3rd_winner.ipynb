{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dacon 월간 데이콘 1 반도체 박막 두께 분석\n",
    "제로콜라맛있다\n",
    "2020년 02 월 04 일 (제출날짜)\n",
    "모델링 코드 작성방법\n",
    "A 코드 관련\n",
    "\n",
    "1) 입상자는 코드 제출 필수. 제출 코드는 예측 결과를 리더보드 점수로 복원할 수 있어야 함\n",
    "\n",
    "2) 코드 제출시 확장자가 R user는 R or .rmd. Python user는 .py or .ipynb\n",
    "\n",
    "3) 코드에 ‘/data’ 데이터 입/출력 경로 포함 제출 or R의 경우 setwd(\" \"), python의 경우 os.chdir을 활용하여 경로 통일\n",
    "\n",
    "4) 전체 프로세스를 일목요연하게 정리하여 주석을 포함하여 하나의 파일로 제출\n",
    "\n",
    "5) 모든 코드는 오류 없이 실행되어야 함(라이브러리 로딩 코드 포함되어야 함).\n",
    "\n",
    "6) 코드와 주석의 인코딩은 모두 UTF-8을 사용하여야 함\n",
    "\n",
    "B 외부 데이터 관련\n",
    "\n",
    "1) 외부 공공 데이터 (날씨 정보 등) 사용이 가능하나, 코드 제출 시 함께 제출\n",
    "\n",
    "2) 공공 데이터 외의 외부 데이터는 법적인 제약이 없는 경우에만 사용 가능\n",
    "\n",
    "3) 외부 데이터를 크롤링할 경우, 크롤링 코드도 함께 제출\n",
    "\n",
    "1. 라이브러리 및 데이터\n",
    "Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU : Tesla V100\n",
    "# Tensorflow 1.14\n",
    "# keras 2.24\n",
    "# numpy 1.17.3\n",
    "# pandas 0.25.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # load and analysis\n",
    "import numpy as np # load and analysis\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import gc\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "# model library\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.models import Model, load_model\n",
    "from keras import optimizers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint,Callback, EarlyStopping\n",
    "from swa.keras import SWA # swa optimizer - https://pypi.org/project/keras-swa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Load\n",
    "TRAIN_PATH = './data/train.csv'\n",
    "TEST_PATH = './data/test.csv'\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_PATH)\n",
    "df_test = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate scheduler 구현\n",
    "# lr을 cyclic하게 변환해줌\n",
    "import math\n",
    "from keras.callbacks import Callback\n",
    "class CosineAnnealingScheduler(Callback):\n",
    "    \"\"\"Cosine annealing scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n",
    "        super(CosineAnnealingScheduler, self).__init__()\n",
    "        self.T_max = T_max\n",
    "        self.eta_max = eta_max\n",
    "        self.eta_min = eta_min\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose &gt; 0:\n",
    "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
    "                  'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gelu activation function -  Gaussian Error Linear Units (GELUs)\n",
    "# https://arxiv.org/abs/1606.08415\n",
    "\n",
    "from keras.layers import Activation\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import tensorflow as tf\n",
    "class Gelu(Activation):\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Gelu, self).__init__(activation, **kwargs)\n",
    "        self.__name__='gelu'\n",
    "        \n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "\n",
    "get_custom_objects().update({'gelu': Gelu(gelu)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 데이터 전처리\n",
    "Data Cleansing & Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 별도의 전처리 없음\n",
    "y_columns = ['layer_1', 'layer_2', 'layer_3', 'layer_4']\n",
    "X_train = df_train.drop(columns=y_columns)\n",
    "y_train = df_train[y_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 탐색적 자료분석\n",
    "Exploratory Data Analysis\n",
    "여러가지 모델을 설계해 보면서 탐구 해보았지만 최종 결과에는 큰지장이 없어 제거함\n",
    "\n",
    "NeuralNet 모델을 사용하는 것이 맞다는 결론에 도달해 모든 피쳐를 사용하는 모델을 사용하기로함\n",
    "\n",
    "4. 변수 선택 및 모델 구축\n",
    "Feature Engineering & Initial Modeling\n",
    "최종모델 선택에 도달하기까지의 모델 구현 과정의 아이디어를 기입하였습니다.\n",
    "\n",
    "1) Feature 추가\n",
    "\n",
    "모든 layer의 두께의 합을 예측 피쳐로 추가함 -> 결과에 큰 차이가 없어 최종 모델에서 제거\n",
    "2) Dropout 조절\n",
    "\n",
    "Underfit 되는 현상을 보고 0.5 에서 0.01까지 계속 낮추며 실험을함, 낮출수록 성능 증가\n",
    "실험을 하다보니 training set에 overfit되지 않는 모습을 보게되어 매우 작은 값으로 선택 (validset과 leaderboard상의 점수가 훨씬 높게 나옴)\n",
    "3) Underfit\n",
    "\n",
    "Layer를 깊고 크게 쌓다보니 학습이 잘 되지않아 SWA, CosineAnnealing Scheduler, Gelu 등을 사용하여 최대한 학습하려고 함\n",
    "특히 Batchnorm 추가 뒤에 성능이 크게 증가\n",
    "4) K-Fold Ensemble\n",
    "\n",
    "구조가 다른 여러 모델을 사용해보았지만 크게 성능향상이 되지 않아 단일 모델의 K-Fold ensemble을 하였음\n",
    "5) 기타 실험 모델들\n",
    "\n",
    "Batchnorm 추가 이후 0.8 -> 0.4 아래로 큰 향상\n",
    "CNN base - 0.44\n",
    "Layer-sum feature 추가 모델 - 0.38\n",
    "기타 Boosting Tree model - 1점대"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 모델 학습 및 검증\n",
    "Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "lr_d = 0.0\n",
    "patience = 200\n",
    "dr_rate = 0.01\n",
    "kfold = 13\n",
    "kf = KFold(n_splits=kfold, shuffle=True, random_state=7777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "for enum, (train_index,valid_index) in enumerate(kf.split(X_train,y_train)):\n",
    "    file_path = f\"bn_swa_nn_best_model_fold_{enum}.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor=\"val_loss\", verbose=1,save_best_only=True, mode=\"min\")\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience)\n",
    "    cosine_scheduler = CosineAnnealingScheduler(T_max=100, eta_max=6e-4, eta_min=3e-5)\n",
    "    swa = SWA(start_epoch=20, lr_schedule='manual', swa_lr=3e-4, swa_freq=5, verbose=1,batch_size=4096)\n",
    "\n",
    "    kf_x_train = X_train.iloc[train_index]\n",
    "    xf_y_train = y_train.iloc[train_index]\n",
    "    \n",
    "    kf_x_val = X_train.iloc[valid_index]\n",
    "    kf_y_val = y_train.iloc[valid_index]\n",
    "\n",
    "    inp = Input(shape = (226,))\n",
    "    x = Dense(2017, activation=None)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = Dropout(dr_rate)(x)\n",
    "    x = Dense(2013, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = Dropout(dr_rate)(x)\n",
    "    x = Dense(1027, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = Dropout(dr_rate)(x)\n",
    "    x = Dense(1023, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = Dropout(dr_rate)(x)\n",
    "    x = Dense(517, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = Dropout(dr_rate)(x)\n",
    "    x = Dense(509, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = Dropout(dr_rate)(x)\n",
    "    x = Dense(503, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = Dropout(dr_rate)(x)\n",
    "    x = Dense(307, activation='gelu')(x)\n",
    "    x = Dropout(dr_rate)(x)\n",
    "    x_1 = Dense(1, activation=\"relu\", name='layer_1')(x)\n",
    "    x_2 = Dense(1, activation=\"relu\", name='layer_2')(x)\n",
    "    x_3 = Dense(1, activation=\"relu\", name='layer_3')(x)\n",
    "    x_4 = Dense(1, activation=\"relu\", name='layer_4')(x)\n",
    "\n",
    "\n",
    "    model = Model(inputs=inp, outputs=[x_1,x_2,x_3,x_4])\n",
    "    model.summary()\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=lr, decay=lr_d))\n",
    "    model.fit(kf_x_train, \n",
    "             [np.array(xf_y_train)[:,0],np.array(xf_y_train)[:,1],np.array(xf_y_train)[:,2],np.array(xf_y_train)[:,3]],\n",
    "              batch_size=4096, epochs=10000,\n",
    "              validation_data = [kf_x_val, [np.array(kf_y_val)[:,0],np.array(kf_y_val)[:,1],np.array(kf_y_val)[:,2],\n",
    "                                            np.array(kf_y_val)[:,3]]],\n",
    "              verbose=2, callbacks=[early_stop,check_point,cosine_scheduler,swa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.iloc[:,1:]\n",
    "\n",
    "pred_test = np.zeros((len(X_test),4))\n",
    "\n",
    "for i in range(kfold):\n",
    "    print(i)\n",
    "    model = load_model(\"./bn_swa_nn_best_model_fold_{}.hdf5\".format(i))\n",
    "    pred = np.array(model.predict(X_test))\n",
    "    pred_test += np.transpose(pred)[0]\n",
    "    \n",
    "pred_test = pred_test /kfold\n",
    "sample_sub = pd.read_csv('./data/sample_submission.csv', index_col=0)\n",
    "submission = sample_sub+pred_test\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 결과 및 결언\n",
    "Conclusion & Discussion\n",
    "데이터셋의 수도 많고 균일한 데이터였지만 training set에 underfit 되는 것을 보니 아직 모델이 부족하거나 데이터가 불완전 한것으로 생각됨\n",
    "\n",
    "SWA를 사용시에 SGD를 optimizer로 택해야 하지만 너무느려서 Adam으로 학습을 함\n",
    "\n",
    "기본적인 NN모델의 성능이 높아 CNN 구조를 깊게 실험해 보지 못한 것이 아쉬움\n",
    "\n",
    "결과적으로는 그래도 많이 실험해보며 재미있게 참여했습니다 !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
