{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dacon 월간 데이콘 Stage1 반도체 박막두께 분석 모델링 경진대회\n",
    "Context_KKP\n",
    "2020년 2월 4일\n",
    "(코드 추출 시간 : 2020/02/04 17:39_\n",
    "모델링 코드 작성방법\n",
    "A 코드 관련\n",
    "\n",
    "1) 입상자는 코드 제출 필수. 제출 코드는 예측 결과를 리더보드 점수로 복원할 수 있어야 함\n",
    "\n",
    "2) 코드 제출시 확장자가 R user는 R or .rmd. Python user는 .py or .ipynb\n",
    "\n",
    "3) 코드에 ‘/data’ 데이터 입/출력 경로 포함 제출 or R의 경우 setwd(\" \"), python의 경우 os.chdir을 활용하여 경로 통일\n",
    "\n",
    "4) 전체 프로세스를 일목요연하게 정리하여 주석을 포함하여 하나의 파일로 제출\n",
    "\n",
    "5) 모든 코드는 오류 없이 실행되어야 함(라이브러리 로딩 코드 포함되어야 함).\n",
    "\n",
    "6) 코드와 주석의 인코딩은 모두 UTF-8을 사용하여야 함\n",
    "\n",
    "B 외부 데이터 관련\n",
    "\n",
    "1) 외부 공공 데이터 (날씨 정보 등) 사용이 가능하나, 코드 제출 시 함께 제출\n",
    "\n",
    "2) 공공 데이터 외의 외부 데이터는 법적인 제약이 없는 경우에만 사용 가능\n",
    "\n",
    "3) 외부 데이터를 크롤링할 경우, 크롤링 코드도 함께 제출\n",
    "\n",
    "1. 라이브러리 및 데이터\n",
    "Library & Data\n",
    "#GPU : Nvidia Tesla V100\n",
    "\n",
    "#pytorch : 1.3.1\n",
    "\n",
    "#torchvision : 0.4.2\n",
    "\n",
    "#pandas : 0.24.2\n",
    "\n",
    "#numpy : 1.15.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library 예시\n",
    "\n",
    "import math\n",
    "import time\n",
    "from itertools import chain\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "# 모델 학습을 위해 CUDA 환경 설정. : 지피유 설정\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 데이터 전처리\n",
    "Data Cleansing & Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(810000, 230)\n",
      "train file saved....\n"
     ]
    }
   ],
   "source": [
    "# 별도의 데이터 Pre-Processing 과정은 없고 모델 훈련시 검증을 위해 train 중 10000개를 validation 용으로 분리.\n",
    "# 새로 만든 train.csv는 train1.csv, validation은 val.csv로 저장.\n",
    "# dataframe.sample(frac=1) 을 통해 셔플.\n",
    "path_train = 'train.csv'\n",
    "path_test = 'test.csv'\n",
    "layers = [['layer_1','layer_2','layer_3','layer_4'], [str(i) for i in np.arange(0,226).tolist()]]\n",
    "layers = list(chain(*layers))\n",
    "\n",
    "train = pd.read_csv(path_train)\n",
    "print(train.shape)\n",
    "train = train.sample(frac=1)\n",
    "rows, cols = train.shape\n",
    "\n",
    "train1 = train.iloc[:rows - 10000,:]\n",
    "train1 = train1.values\n",
    "train1 = pd.DataFrame(data=train1,columns=layers)\n",
    "train1.to_csv('train1.csv', index_label='id')\n",
    "\n",
    "print(\"train file saved....\")\n",
    "val = train.iloc[rows - 10000:,:]\n",
    "val = val.values\n",
    "val = pd.DataFrame(data=val,columns=layers)\n",
    "val.to_csv('val.csv', index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로 만든 train/ val 모델 학습 데이터 경로를 설정.\n",
    "train_path = 'train1.csv'\n",
    "val_path = 'val.csv'\n",
    "\n",
    "lr = 1e-03\n",
    "adam_epsilon = 1e-06\n",
    "epochs = 100\n",
    "batch_size = 2048\n",
    "warmup_step = 2000\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "class PandasDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super(PandasDataset, self).__init__()\n",
    "        train = pd.read_csv(path).iloc[:,1:]\n",
    "        self.train_X, self.train_Y = train.iloc[:,4:], train.iloc[:,0:4]\n",
    "        self.tmp_x , self.tmp_y = self.train_X.values, self.train_Y.values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'X':torch.from_numpy(self.tmp_x)[idx],\n",
    "            'Y':torch.from_numpy(self.tmp_y)[idx]\n",
    "        }\n",
    "            \n",
    "train_dataset = PandasDataset(train_path)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,  num_workers=4)\n",
    "\n",
    "val_dataset = PandasDataset(val_path)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,  num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 탐색적 자료분석\n",
    "Exploratory Data Analysis\n",
    "1) 4개의 layer_1, layer_2, layer_3, layer_4의 두께를 구하는 방법이라 마지막 Dense unit을 1로\n",
    "2) 해서 각각의 두께를 구하는 모델을 구성해보았으나 결과는 좋지 않음.\n",
    "3) 특별한 탐색적 자료 분석 없음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 변수 선택 및 모델 구축\n",
    "Feature Engineering & Initial Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Modeling\n",
    "\n",
    "Learning rate : 다양한 learning rate 스케줄링. Warmup 스케줄링이 train, val loss 줄이는데 가장 효과적.\n",
    "Activation function : ReLU, ELU, CeLU, GeLU 사용. 음의 영역에서는 0의 값을 가지는 relu 대신 ELU, CeLU, GeLU등을 사용. relu보다는 효과적.\n",
    "Optimizer : adam, adamW.\n",
    "Batch size : batch size는 10000으로 시작하였으나 10000보다 작은 값에서 효과적.\n",
    "Hidden node, layer depth : node 수는 Nh = Ns/(a * (Ni + No))에 따라 초기 모델링에서는 약 2500개 정도로 하였으나 실험적으로 많은\n",
    "node가 train, val loss 줄이는데 효과적.\n",
    "(Nh : number of hidden , Ni : Number of input, No : Number of output, a : 2 - 10)\n",
    "Loss funciotion : L1 Loss\n",
    "BatchNorm : 모든 layer에 batchNorm 적용\n",
    "MLP Model : 단순한 MLP 모델로는 한계를 느껴 MLP의 복잡성을 증가시키는 방향으로 설계\n",
    "        (1) 노드 수를 증가시켰다 줄이는 방식(UP-block, DOWN-block)\n",
    "        (2) Down-block에서 skip connection과 layer norm 적용\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-5):\n",
    "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
    "        \"\"\"\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.weight.data.fill_(1.0)\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.weight * x + self.bias\n",
    "class skipConnectionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(skipConnectionModel, self).__init__()\n",
    "        \n",
    "        self.ln = LayerNorm(10000)\n",
    "        self.ln1 = LayerNorm(7000)\n",
    "        self.ln2 = LayerNorm(4000)\n",
    "        self.ln3 = LayerNorm(2000)\n",
    "        \n",
    "        self.upblock1 = nn.Sequential(nn.Linear(226, 2000),GELU(),nn.BatchNorm1d(2000))\n",
    "        self.upblock2 = nn.Sequential(nn.Linear(2000,4000),GELU(),nn.BatchNorm1d(4000))\n",
    "        self.upblock3 = nn.Sequential(nn.Linear(4000,7000), GELU(),nn.BatchNorm1d(7000))\n",
    "        self.upblock4 = nn.Sequential(nn.Linear(7000,10000),GELU(),nn.BatchNorm1d(10000))\n",
    "\n",
    "        self.downblock1 = nn.Sequential(nn.Linear(10000, 7000),GELU(),nn.BatchNorm1d(7000))\n",
    "        self.downblock2 = nn.Sequential(nn.Linear(7000, 4000),GELU(),nn.BatchNorm1d(4000))\n",
    "        self.downblock3 = nn.Sequential(nn.Linear(4000, 2000),GELU(),nn.BatchNorm1d(2000))\n",
    "        self.downblock4 = nn.Sequential(nn.Linear(2000, 300),GELU(),nn.BatchNorm1d(300))\n",
    "        \n",
    "        self.fclayer = nn.Sequential(nn.Linear(300,4))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        upblock1_out = self.upblock1(x)\n",
    "        upblock2_out = self.upblock2(upblock1_out)\n",
    "        upblock3_out = self.upblock3(upblock2_out)\n",
    "        upblock4_out = self.upblock4(upblock3_out)\n",
    "        \n",
    "        downblock1_out = self.downblock1(self.ln(upblock4_out))\n",
    "        skipblock1 = downblock1_out + upblock3_out\n",
    "        downblock2_out = self.downblock2(self.ln1(skipblock1))\n",
    "        skipblock2 = downblock2_out + upblock2_out\n",
    "        downblock3_out = self.downblock3(self.ln2(skipblock2))\n",
    "        skipblock3 = downblock3_out + upblock1_out\n",
    "        downblock4_out = self.downblock4(self.ln3(skipblock3))\n",
    "        \n",
    "        output = self.fclayer(downblock4_out)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def get_constant_schedule(optimizer, last_epoch=-1):\n",
    "    \"\"\" Create a schedule with a constant learning rate.\n",
    "    \"\"\"\n",
    "    return LambdaLR(optimizer, lambda _: 1, last_epoch=last_epoch)\n",
    "\n",
    "\n",
    "def get_constant_schedule_with_warmup(optimizer, num_warmup_steps, last_epoch=-1):\n",
    "    \"\"\" Create a schedule with a constant learning rate preceded by a warmup\n",
    "    period during which the learning rate increases linearly between 0 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1.0, num_warmup_steps))\n",
    "        return 1.0\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "\n",
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\n",
    "    \"\"\" Create a schedule with a learning rate that decreases linearly after\n",
    "    linearly increasing during a warmup period.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, last_epoch=-1):\n",
    "    \"\"\" Create a schedule with a learning rate that decreases following the\n",
    "    values of the cosine function between 0 and `pi * cycles` after a warmup\n",
    "    period during which it increases linearly between 0 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "def get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps, num_training_steps, num_cycles=1.0, last_epoch=-1\n",
    "):\n",
    "    \"\"\" Create a schedule with a learning rate that decreases following the\n",
    "    values of the cosine function with several hard restarts, after a warmup\n",
    "    period during which it increases linearly between 0 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        if progress >= 1.0:\n",
    "            return 0.0\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * ((float(num_cycles) * progress) % 1.0))))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "class AdamW(Optimizer):\n",
    "    \"\"\" Implements Adam algorithm with weight decay fix.\n",
    "    Parameters:\n",
    "        lr (float): learning rate. Default 1e-3.\n",
    "        betas (tuple of 2 floats): Adams beta parameters (b1, b2). Default: (0.9, 0.999)\n",
    "        eps (float): Adams epsilon. Default: 1e-6\n",
    "        weight_decay (float): Weight decay. Default: 0.0\n",
    "        correct_bias (bool): can be set to False to avoid correcting bias in Adam (e.g. like in Bert TF repository). Default True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0, correct_bias=True):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(eps))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, correct_bias=correct_bias)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # In-place operations to update the averages at the same time\n",
    "                exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)\n",
    "                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "\n",
    "                step_size = group[\"lr\"]\n",
    "                if group[\"correct_bias\"]:  # No bias correction for Bert\n",
    "                    bias_correction1 = 1.0 - beta1 ** state[\"step\"]\n",
    "                    bias_correction2 = 1.0 - beta2 ** state[\"step\"]\n",
    "                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                # Just adding the square of the weights to the loss function is *not*\n",
    "                # the correct way of using L2 regularization/weight decay with Adam,\n",
    "                # since that will interact with the m and v parameters in strange ways.\n",
    "                #\n",
    "                # Instead we want to decay the weights in a manner that doesn't interact\n",
    "                # with the m/v parameters. This is equivalent to adding the square\n",
    "                # of the weights to the loss with plain (non-momentum) SGD.\n",
    "                # Add weight decay at the end (fixed version)\n",
    "                if group[\"weight_decay\"] > 0.0:\n",
    "                    p.data.add_(-group[\"lr\"] * group[\"weight_decay\"], p.data)\n",
    "\n",
    "        return loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e67e5e33ea35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskipConnectionModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 모델을 GPU 메모리에 올림.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    190\u001b[0m             raise RuntimeError(\n\u001b[0;32m    191\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0m_cudart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "model = skipConnectionModel()\n",
    "model = model.to(device) # 모델을 GPU 메모리에 올림."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 모델 학습 및 검증\n",
    "Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "모델 학습\n",
    "\"\"\"\n",
    "\n",
    "total_step = len(train_loader) * epochs\n",
    "print(f\"Total step is....{total_step}\") # 모델이 학습하는 전체 step 계산.\n",
    "\n",
    "# 옵티마이저와 스케줄러의 파라미터들을 정의.\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"] # decay하지 않을 영역 지정.\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=adam_epsilon)\n",
    "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_step, num_training_steps=total_step\n",
    ")\n",
    "\n",
    "# train loss와 val loss 지정.\n",
    "total_loss = 0.0\n",
    "total_val_loss = 0.0\n",
    "\n",
    "# 모델 이름을 위해서 변수 만듦.\n",
    "version = time.localtime()[3:5]\n",
    "curr_lr = lr\n",
    "\n",
    "n_val_loss = 10000000. # 가장 낮은 validation loss를 저장하기 위해서 변수 설정.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0 \n",
    "    total_val_loss = 0\n",
    "    for i, data in enumerate(tqdm(train_loader, desc='*********Train mode*******')):  # train 데이터를 부르고 학습.\n",
    "        # forward pass\n",
    "        pred = model(data['X'].float().to(device))\n",
    "        loss = loss_fn(pred, data['Y'].float().to(device))\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad() # optimizer 객체 사용해서 학습 가능한 가중치 변수에 대한 모든 변화도를 0으로 만듦\n",
    "        loss.backward() \n",
    "        optimizer.step() # update optimizer params\n",
    "        scheduler.step() # update scheduler params\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    print (\"Epoch [{}/{}], Train Loss: {:.4f}\".format(epoch+1, epochs, train_loss))\n",
    "\n",
    "    # evaluation\n",
    "    # validation 데이터를 부르고 epoch 마다 학습된 모델을 부르고 평가.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(val_loader, desc='*********Evaluation mode*******')):\n",
    "            pred = model(data['X'].float().to(device))\n",
    "            loss_val = loss_fn(pred, data['Y'].float().to(device))\n",
    "            \n",
    "            total_val_loss += loss_val.item()\n",
    "    val_loss = total_val_loss / len(val_loader)\n",
    "    print (\"Epoch [{}/{}], Eval Loss: {:.4f}\".format(epoch+1, epochs, val_loss))\n",
    "    \n",
    "    # best model을 저장.\n",
    "    if val_loss < n_val_loss:\n",
    "        n_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'test_{version}_{lr}_{epochs}.pth')\n",
    "        print(\"Best Model saved......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "모델 테스트\n",
    "\"\"\"\n",
    "\n",
    "test_model = skipConnectionModel()\n",
    "\n",
    "# test 파일 경로 및 test 데이터 로드\n",
    "path_test = 'test.csv'\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, path_test):\n",
    "        super(TestDataset, self).__init__()\n",
    "        test = pd.read_csv(path_test)\n",
    "        self.test_X = test.iloc[:,1:]\n",
    "        self.tmp_x = self.test_X.values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.test_X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.tmp_x)[idx]\n",
    "    \n",
    "test_data = TestDataset(path_test)\n",
    "test_loader = DataLoader(test_data, batch_size=10000,  num_workers=4)\n",
    "\n",
    "# 모델에 학습된 가중치를 업로드.\n",
    "weights = torch.load(f'test_{version}_{lr}_{epochs}.pth', map_location='cuda:0')\n",
    "test_model.load_state_dict(weights)\n",
    "test_model = test_model.to(device)\n",
    "test_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        outputs = test_model(data.float())\n",
    "pred_test = outputs\n",
    "\n",
    "sample_sub = pd.read_csv('sample_submission.csv', index_col=0)\n",
    "layers = ['layer_1','layer_2','layer_3','layer_4']\n",
    "submission = sample_sub.values + pred_test.cpu().numpy()\n",
    "\n",
    "submission = pd.DataFrame(data=submission,columns=layers)\n",
    "submission.to_csv(f'test_{version}_{lr}_{epochs}.csv', index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# + Self evaluation and Ensemble\n",
    "# Dacon의 제출하기를 통해 측정한 mae값 중 가장 낮은 mae csv 파일과 다른 파라미터 적용으로 훈련한 모델과 mae 비교를 통해\n",
    "# 대략적인 test mae를 예상한 뒤 제출하기 하여 3번 제출할 수 있는 기회를 최대한 살림.\n",
    "# 다양한 파라미터 적용을 통한 모델들을 아래의 en함수를 통해 합친 뒤 평균을 구하여 제출\n",
    "# (추가하는 csv 파일의 수에 따라 en함수의 함수가 받는 csv 파일 개수 증가 및 코드 수정 필요.)\n",
    "def mae(best_path, my_path):\n",
    "    best = pd.read_csv(best_path)\n",
    "    best_value = best.iloc[:,1:].values\n",
    "    value = pd.read_csv(my_path)\n",
    "    my_value = value.iloc[:,1:].values\n",
    "    abs_value = abs(best_value - my_value)\n",
    "    size = abs_value.shape\n",
    "    return sum(sum(abs_value) / (size[0]*size[1]))\n",
    "\n",
    "def en(best_path, my_path):\n",
    "    best = pd.read_csv(best_path)\n",
    "    best_value = best.iloc[:,1:].values\n",
    "    value = pd.read_csv(my_path)\n",
    "    my_value = value.iloc[:,1:].values\n",
    "    return (my_value + best_value)/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 결과 및 결언\n",
    "Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "초기 base line model을 만들고 난 뒤 lr scheduler, optimizer, activation function, layer depth 등\n",
    "다양한 파라미터 조정을 통해서 base line model보다 낮은 mae를 가질 경우 앙상블 할 목록에 추가하여\n",
    "다양한 앙상블 조합을 선택했습니다.(최종 제출에는 12개 모델 앙상블)\n",
    "\n",
    "단순한 MLP 모델을 통해서는 loss감소에 한계를 느껴 MLP의 복잡성을 증가시키는 Up-block, Down-block, Skip-connection등의 아이디어를\n",
    "적용한 결과 좋은 결과를 얻은 것 같습니다.\n",
    "\n",
    "Skip-connection은 ResNet에서 사용되어 이전 feature map에 대한 정보를 다음 layer에 제공하므로써 영상쪽에서 효과적으로 쓰이고 있으며\n",
    "논문에 따르면 skip connection 의 또다른 효과는 이전 feature와 gradient 등의 정보의 통로가 확장되어 앙상블과 같은 효과가 있다고 하는데,\n",
    "MLP에 적용했을 때는 어떤 원리로 효과가 있을지 아시는 분은 댓글로 공유해주시면 감사하겠습니다.\n",
    "\n",
    "감사합니다.\n",
    "\n",
    "궁금하신 내용은 댓글로 남겨주세요.\n",
    "\n",
    "(한달간 함께 고생한 팀원분들 고생많았고 감사합니다.)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
