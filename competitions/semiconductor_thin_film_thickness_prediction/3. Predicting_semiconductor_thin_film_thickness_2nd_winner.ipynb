{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dacon 월간 데이콘 1 반도체 박막 두께 분석\n",
    "Overfit\n",
    "2020년 02월 03일 (제출날짜)\n",
    "모델링 코드 작성방법\n",
    "A 코드 관련\n",
    "\n",
    "1) 입상자는 코드 제출 필수. 제출 코드는 예측 결과를 리더보드 점수로 복원할 수 있어야 함\n",
    "\n",
    "2) 코드 제출시 확장자가 R user는 R or .rmd. Python user는 .py or .ipynb\n",
    "\n",
    "3) 코드에 ‘/data’ 데이터 입/출력 경로 포함 제출 or R의 경우 setwd(\" \"), python의 경우 os.chdir을 활용하여 경로 통일\n",
    "\n",
    "4) 전체 프로세스를 일목요연하게 정리하여 주석을 포함하여 하나의 파일로 제출\n",
    "\n",
    "5) 모든 코드는 오류 없이 실행되어야 함(라이브러리 로딩 코드 포함되어야 함).\n",
    "\n",
    "6) 코드와 주석의 인코딩은 모두 UTF-8을 사용하여야 함\n",
    "\n",
    "B 외부 데이터 관련\n",
    "\n",
    "1) 외부 공공 데이터 (날씨 정보 등) 사용이 가능하나, 코드 제출 시 함께 제출\n",
    "\n",
    "2) 공공 데이터 외의 외부 데이터는 법적인 제약이 없는 경우에만 사용 가능\n",
    "\n",
    "3) 외부 데이터를 크롤링할 경우, 크롤링 코드도 함께 제출\n",
    "\n",
    "1. 라이브러리 및 데이터\n",
    "Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시스템 환경은 다음과 같음.\n",
    "\n",
    "# OS : Ubuntu 18.04.3 LTS\n",
    "# CPU : Intel(R) Xeon(R) CPU @ 2.30GHz (4 CPUs.)\n",
    "# GPU : Nvidia Tesla T4 (1 GPU.)\n",
    "\n",
    "# Python Version : 3.7.6\n",
    "# Nvidia Driver Version : 440.33.01\n",
    "# CUDA Version (PyTorch) : 10.1.243\n",
    "# cuDNN Version (PyTorch) : 7.6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np  # 1.18.1\n",
    "from numpy.random import shuffle\n",
    "import pandas as pd  # 0.25.3\n",
    "import torch  # 1.4.0\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data.sampler import Sampler, SequentialSampler\n",
    "from torch.backends import cudnn\n",
    "\n",
    "#실행 파일과 같은 directory에 'train.csv'와 'test.csv'를 포함하는 'Data' 폴더 필요함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 데이터 전처리\n",
    "Data Cleansing & Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 데이터를 전부 활용하였으며 전처리 하지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 탐색적 자료분석\n",
    "Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas를 이용하여 correlation matrix를 계산하고 시각화함.\n",
    "# (예측 결과를 복원하는 것과는 무관함으로 code는 생략함.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 변수 선택 및 모델 구축\n",
    "Feature Engineering & Initial Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully-connected NN(layer 5개 미만)을 이용한 초기 실험에서 under-fitting되는 문제를 확인함.\n",
    "# depth를 키운 fully-connected NN을 기본적으로 이용하였고 이를 ensemble하여 성능을 개선함.\n",
    "# (예측 결과를 복원하는 것과는 무관함으로 code는 생략함.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 모델 학습 및 검증\n",
    "Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 부분에 대한 code 설명을 다음과 같음.\n",
    "\n",
    "# ContinuousBatchSampler는 mini-batch의 크기가 항상 일정하게 유지되도록 하는 BatchSampler임.\n",
    "# 대회 목표와 같은 L1Loss를 이용함.\n",
    "# SGD optimizer를 이용함.\n",
    "# CosineAnnealingWarmRestarts를 이용하여 learning rate를 조절함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 1\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class ContinuousBatchSampler(Sampler):\n",
    "    def __init__(self, sampler, batch_size, drop_last):\n",
    "        self.sampler = sampler\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.from_last_epoch = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        idx_from_sampler = set(self.sampler)\n",
    "        idx_to_exclude = set(self.from_last_epoch)\n",
    "        idx_after_exclusion = sorted(list(idx_from_sampler - idx_to_exclude))\n",
    "        shuffle(idx_after_exclusion)\n",
    "        first_batch = self.from_last_epoch + idx_after_exclusion[:self.batch_size - len(self.from_last_epoch)]\n",
    "        yield first_batch\n",
    "        idx_of_left = sorted(idx_after_exclusion[self.batch_size - len(self.from_last_epoch):] + list(idx_to_exclude))\n",
    "        shuffle(idx_of_left)\n",
    "        batch = []\n",
    "        for idx in idx_of_left:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if not self.drop_last:\n",
    "            self.from_last_epoch = batch.copy()\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size\n",
    "        else:\n",
    "            return (len(self.sampler) + len(self.from_last_epoch)) // self.batch_size\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "num_epochs = 1000\n",
    "batch_size = 2048\n",
    "initial_learning_rate = 0.2\n",
    "loader_params = {'num_workers': 8, 'pin_memory': True}\n",
    "\n",
    "train_data = np.array(pd.read_csv('./Data/train.csv'), dtype=np.float32)\n",
    "X_train = torch.tensor(train_data[:, 4:], dtype=torch.float32)\n",
    "y_train = torch.tensor(train_data[:, :4], dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_sampler=ContinuousBatchSampler(\n",
    "    sampler=SequentialSampler(range(len(dataset))), batch_size=batch_size, drop_last=False), **loader_params)\n",
    "\n",
    "prediction = np.zeros((10000, 4), dtype=np.float32)\n",
    "logging_term = 1000\n",
    "logging_total = int(810000 * num_epochs / batch_size)\n",
    "\n",
    "for model_no in ['Model_01', 'Model_02', 'Model_03', 'Model_04', 'Model_05',\n",
    "                 'Model_06', 'Model_07', 'Model_08', 'Model_09', 'Model_10',\n",
    "                 'Model_11', 'Model_12', 'Model_13', 'Model_14', 'Model_15',\n",
    "                 'Model_16', 'Model_17', 'Model_18', 'Model_19', 'Model_20']:\n",
    "\n",
    "    net = nn.Sequential(\n",
    "        nn.BatchNorm1d(226),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(226, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 4)\n",
    "    )\n",
    "\n",
    "    model = net.to(device)\n",
    "    running_loss = 0.\n",
    "    running_counter = 0\n",
    "\n",
    "    criterion = torch.nn.L1Loss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=initial_learning_rate, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=250, T_mult=1, eta_min=0.005,\n",
    "                                                               last_epoch=-1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        for xx, yy in train_loader:\n",
    "\n",
    "            xx, yy = xx.to(device), yy.to(device)\n",
    "            with torch.no_grad():\n",
    "                xx += torch.randn((xx.shape[0], 226), device='cuda:0') * 0.003\n",
    "\n",
    "            out = model(xx)\n",
    "            loss = criterion(out, yy)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_counter += 1\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if running_counter % logging_term == 0:\n",
    "                print(model_no + ' (iter {:6d}/{:6d}) {:.4f}'.format(running_counter, logging_total,\n",
    "                                                                     running_loss / logging_term))\n",
    "                running_loss = 0.\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    output = model(\n",
    "        torch.tensor(np.array(pd.read_csv('./Data/test.csv'), dtype=np.float32))[:, 1:].to(device))\n",
    "    output = np.array(output.detach().to('cpu'), dtype=np.float32)\n",
    "    prediction += output * 0.05\n",
    "\n",
    "prediction = pd.DataFrame({'id': np.array(list(range(10000)), dtype=np.int32),\n",
    "                           'layer_1': prediction[:, 0],\n",
    "                           'layer_2': prediction[:, 1],\n",
    "                           'layer_3': prediction[:, 2],\n",
    "                           'layer_4': prediction[:, 3]})\n",
    "prediction.to_csv('./prediction_result_{}.csv'.format(time.strftime('%y%m%d%H%M%S')), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 2\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class ContinuousBatchSampler(Sampler):\n",
    "    def __init__(self, sampler, batch_size, drop_last):\n",
    "        self.sampler = sampler\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.from_last_epoch = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        idx_from_sampler = set(self.sampler)\n",
    "        idx_to_exclude = set(self.from_last_epoch)\n",
    "        idx_after_exclusion = sorted(list(idx_from_sampler - idx_to_exclude))\n",
    "        shuffle(idx_after_exclusion)\n",
    "        first_batch = self.from_last_epoch + idx_after_exclusion[:self.batch_size - len(self.from_last_epoch)]\n",
    "        yield first_batch\n",
    "        idx_of_left = sorted(idx_after_exclusion[self.batch_size - len(self.from_last_epoch):] + list(idx_to_exclude))\n",
    "        shuffle(idx_of_left)\n",
    "        batch = []\n",
    "        for idx in idx_of_left:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if not self.drop_last:\n",
    "            self.from_last_epoch = batch.copy()\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size\n",
    "        else:\n",
    "            return (len(self.sampler) + len(self.from_last_epoch)) // self.batch_size\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "num_epochs = 1000\n",
    "batch_size = 2048\n",
    "initial_learning_rate = 0.16\n",
    "loader_params = {'num_workers': 8, 'pin_memory': True}\n",
    "\n",
    "train_data = np.array(pd.read_csv('./Data/train.csv'), dtype=np.float32)\n",
    "X_train = torch.tensor(train_data[:, 4:], dtype=torch.float32)\n",
    "y_train = torch.tensor(train_data[:, :4], dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_sampler=ContinuousBatchSampler(\n",
    "    sampler=SequentialSampler(range(len(dataset))), batch_size=batch_size, drop_last=False), **loader_params)\n",
    "\n",
    "prediction = np.zeros((10000, 4), dtype=np.float32)\n",
    "logging_term = 1000\n",
    "logging_total = int(810000 * num_epochs / batch_size)\n",
    "\n",
    "for model_no in ['Model_01', 'Model_02', 'Model_03', 'Model_04', 'Model_05', 'Model_06', 'Model_07']:\n",
    "\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(226, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 4)\n",
    "    )\n",
    "\n",
    "    model = net.to(device)\n",
    "    running_loss = 0.\n",
    "    running_counter = 0\n",
    "\n",
    "    criterion = torch.nn.L1Loss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=initial_learning_rate, momentum=0.9, weight_decay=2e-6)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=250, T_mult=1, eta_min=0.005,\n",
    "                                                               last_epoch=-1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        for xx, yy in train_loader:\n",
    "\n",
    "            xx, yy = xx.to(device), yy.to(device)\n",
    "            with torch.no_grad():\n",
    "                xx += torch.randn((xx.shape[0], 226), device='cuda:0') * 0.00325\n",
    "\n",
    "            out = model(xx)\n",
    "            loss = criterion(out, yy)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_counter += 1\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if running_counter % logging_term == 0:\n",
    "                print(model_no + ' (iter {:6d}/{:6d}) {:.4f}'.format(running_counter, logging_total,\n",
    "                                                                     running_loss / logging_term))\n",
    "                running_loss = 0.\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    output = model(\n",
    "        torch.tensor(np.array(pd.read_csv('./Data/test.csv'), dtype=np.float32))[:, 1:].to(device))\n",
    "    output = np.array(output.detach().to('cpu'), dtype=np.float32)\n",
    "    prediction += output * 1 / 7\n",
    "\n",
    "prediction = pd.DataFrame({'id': np.array(list(range(10000)), dtype=np.int32),\n",
    "                           'layer_1': prediction[:, 0],\n",
    "                           'layer_2': prediction[:, 1],\n",
    "                           'layer_3': prediction[:, 2],\n",
    "                           'layer_4': prediction[:, 3]})\n",
    "prediction.to_csv('./prediction_result_{}.csv'.format(time.strftime('%y%m%d%H%M%S')), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 3\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class ContinuousBatchSampler(Sampler):\n",
    "    def __init__(self, sampler, batch_size, drop_last):\n",
    "        self.sampler = sampler\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.from_last_epoch = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        idx_from_sampler = set(self.sampler)\n",
    "        idx_to_exclude = set(self.from_last_epoch)\n",
    "        idx_after_exclusion = sorted(list(idx_from_sampler - idx_to_exclude))\n",
    "        shuffle(idx_after_exclusion)\n",
    "        first_batch = self.from_last_epoch + idx_after_exclusion[:self.batch_size - len(self.from_last_epoch)]\n",
    "        yield first_batch\n",
    "        idx_of_left = sorted(idx_after_exclusion[self.batch_size - len(self.from_last_epoch):] + list(idx_to_exclude))\n",
    "        shuffle(idx_of_left)\n",
    "        batch = []\n",
    "        for idx in idx_of_left:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if not self.drop_last:\n",
    "            self.from_last_epoch = batch.copy()\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size\n",
    "        else:\n",
    "            return (len(self.sampler) + len(self.from_last_epoch)) // self.batch_size\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "num_epochs = 1000\n",
    "batch_size = 2048\n",
    "initial_learning_rate = 0.16\n",
    "loader_params = {'num_workers': 8, 'pin_memory': True}\n",
    "\n",
    "train_data = np.array(pd.read_csv('./Data/train.csv'), dtype=np.float32)\n",
    "X_train = torch.tensor(train_data[:, 4:], dtype=torch.float32)\n",
    "y_train = torch.tensor(train_data[:, :4], dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_sampler=ContinuousBatchSampler(\n",
    "    sampler=SequentialSampler(range(len(dataset))), batch_size=batch_size, drop_last=False), **loader_params)\n",
    "\n",
    "prediction = np.zeros((10000, 4), dtype=np.float32)\n",
    "logging_term = 1000\n",
    "logging_total = int(810000 * num_epochs / batch_size)\n",
    "\n",
    "for model_no in ['Model_01', 'Model_02', 'Model_03', 'Model_04', 'Model_05', 'Model_06', 'Model_07']:\n",
    "\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(226, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 768),\n",
    "        nn.BatchNorm1d(768),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(768, 4)\n",
    "    )\n",
    "\n",
    "    model = net.to(device)\n",
    "    running_loss = 0.\n",
    "    running_counter = 0\n",
    "\n",
    "    criterion = torch.nn.L1Loss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=initial_learning_rate, momentum=0.9, weight_decay=2e-6)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=250, T_mult=1, eta_min=0.005,\n",
    "                                                               last_epoch=-1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        for xx, yy in train_loader:\n",
    "\n",
    "            xx, yy = xx.to(device), yy.to(device)\n",
    "            with torch.no_grad():\n",
    "                xx += torch.randn((xx.shape[0], 226), device='cuda:0') * 0.00325\n",
    "\n",
    "            out = model(xx)\n",
    "            loss = criterion(out, yy)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_counter += 1\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if running_counter % logging_term == 0:\n",
    "                print(model_no + ' (iter {:6d}/{:6d}) {:.4f}'.format(running_counter, logging_total,\n",
    "                                                                     running_loss / logging_term))\n",
    "                running_loss = 0.\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    output = model(\n",
    "        torch.tensor(np.array(pd.read_csv('./Data/test.csv'), dtype=np.float32))[:, 1:].to(device))\n",
    "    output = np.array(output.detach().to('cpu'), dtype=np.float32)\n",
    "    prediction += output * 1 / 7\n",
    "\n",
    "prediction = pd.DataFrame({'id': np.array(list(range(10000)), dtype=np.int32),\n",
    "                           'layer_1': prediction[:, 0],\n",
    "                           'layer_2': prediction[:, 1],\n",
    "                           'layer_3': prediction[:, 2],\n",
    "                           'layer_4': prediction[:, 3]})\n",
    "prediction.to_csv('./prediction_result_{}.csv'.format(time.strftime('%y%m%d%H%M%S')), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 4\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class ContinuousBatchSampler(Sampler):\n",
    "    def __init__(self, sampler, batch_size, drop_last):\n",
    "        self.sampler = sampler\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.from_last_epoch = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        idx_from_sampler = set(self.sampler)\n",
    "        idx_to_exclude = set(self.from_last_epoch)\n",
    "        idx_after_exclusion = sorted(list(idx_from_sampler - idx_to_exclude))\n",
    "        shuffle(idx_after_exclusion)\n",
    "        first_batch = self.from_last_epoch + idx_after_exclusion[:self.batch_size - len(self.from_last_epoch)]\n",
    "        yield first_batch\n",
    "        idx_of_left = sorted(idx_after_exclusion[self.batch_size - len(self.from_last_epoch):] + list(idx_to_exclude))\n",
    "        shuffle(idx_of_left)\n",
    "        batch = []\n",
    "        for idx in idx_of_left:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if not self.drop_last:\n",
    "            self.from_last_epoch = batch.copy()\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size\n",
    "        else:\n",
    "            return (len(self.sampler) + len(self.from_last_epoch)) // self.batch_size\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "num_epochs = 1000\n",
    "batch_size = 2048\n",
    "initial_learning_rate = 0.16\n",
    "loader_params = {'num_workers': 8, 'pin_memory': True}\n",
    "\n",
    "train_data = np.array(pd.read_csv('./Data/train.csv'), dtype=np.float32)\n",
    "X_train = torch.tensor(train_data[:, 4:], dtype=torch.float32)\n",
    "y_train = torch.tensor(train_data[:, :4], dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_sampler=ContinuousBatchSampler(\n",
    "    sampler=SequentialSampler(range(len(dataset))), batch_size=batch_size, drop_last=False), **loader_params)\n",
    "\n",
    "prediction = np.zeros((10000, 4), dtype=np.float32)\n",
    "logging_term = 1000\n",
    "logging_total = int(810000 * num_epochs / batch_size)\n",
    "\n",
    "\n",
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AvgPool1d(kernel_size=3, stride=3, padding=1)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(76, 768),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 768),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 768),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 768),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 768),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 768),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 768),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 768),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], 1, -1)\n",
    "        x = self.avg(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "for model_no in ['Model_01', 'Model_02', 'Model_03', 'Model_04', 'Model_05', 'Model_06', 'Model_07']:\n",
    "\n",
    "    model = net().to(device)\n",
    "    running_loss = 0.\n",
    "    running_counter = 0\n",
    "\n",
    "    criterion = torch.nn.L1Loss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=initial_learning_rate, momentum=0.9, weight_decay=2e-6)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=250, T_mult=1, eta_min=0.005,\n",
    "                                                               last_epoch=-1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        for xx, yy in train_loader:\n",
    "\n",
    "            xx, yy = xx.to(device), yy.to(device)\n",
    "            with torch.no_grad():\n",
    "                xx += torch.randn((xx.shape[0], 226), device='cuda:0') * 0.005\n",
    "\n",
    "            out = model(xx)\n",
    "            loss = criterion(out, yy)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_counter += 1\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if running_counter % logging_term == 0:\n",
    "                print(model_no + ' (iter {:6d}/{:6d}) {:.4f}'.format(running_counter, logging_total,\n",
    "                                                                     running_loss / logging_term))\n",
    "                running_loss = 0.\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    output = model(\n",
    "        torch.tensor(np.array(pd.read_csv('./Data/test.csv'), dtype=np.float32))[:, 1:].to(device))\n",
    "    output = np.array(output.detach().to('cpu'), dtype=np.float32)\n",
    "    prediction += output * 1 / 7\n",
    "\n",
    "prediction = pd.DataFrame({'id': np.array(list(range(10000)), dtype=np.int32),\n",
    "                           'layer_1': prediction[:, 0],\n",
    "                           'layer_2': prediction[:, 1],\n",
    "                           'layer_3': prediction[:, 2],\n",
    "                           'layer_4': prediction[:, 3]})\n",
    "prediction.to_csv('./prediction_result_{}.csv'.format(time.strftime('%y%m%d%H%M%S')), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 4개의 학습 코드를 실행하면 4개의 임시 결과 파일이 생성됨.\n",
    "# 이를 다시 아래의 코드로 ensemble하여 성능을 개선함.\n",
    "# 아래의 코드로 생성된 결과를 최종 제출함.\n",
    "\n",
    "file_list = np.sort(glob('./*.csv'))\n",
    "\n",
    "file_0 = pd.read_csv(file_list[0]).to_numpy()[:, 1:]\n",
    "file_1 = pd.read_csv(file_list[1]).to_numpy()[:, 1:]\n",
    "file_2 = pd.read_csv(file_list[2]).to_numpy()[:, 1:]\n",
    "file_3 = pd.read_csv(file_list[3]).to_numpy()[:, 1:]\n",
    "\n",
    "prediction = (file_0 + file_1 + file_2 + file_3) / 4\n",
    "\n",
    "prediction = pd.DataFrame({'id': np.array(list(range(10000)), dtype=np.int32),\n",
    "                           'layer_1': prediction[:, 0],\n",
    "                           'layer_2': prediction[:, 1],\n",
    "                           'layer_3': prediction[:, 2],\n",
    "                           'layer_4': prediction[:, 3]})\n",
    "prediction.to_csv('../prediction_result_{}.csv'.format(time.strftime('%y%m%d%H%M%S')), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 결과 및 결언\n",
    "Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 많은 수의 model을 학습하여 ensemble함으로 확률적(stochastically)으로 \n",
    "# under-fitting 문제를 개선하고 안정적인 model을 학습할 수 있었음.\n",
    "\n",
    "# 시간 문제로 많은 수의 실험을 진행하지는 못했지만 model의(depth, width 등)을\n",
    "# 더 넓은 범위에서 찾으면 존재하는 under-fitting 문제를 해결할 수 있을 것으로 사료됨.\n",
    "# 또한, 최근 연구되고 있는 AutoML 등을 기법을 이용하는 것을 고려할 수 있음."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
