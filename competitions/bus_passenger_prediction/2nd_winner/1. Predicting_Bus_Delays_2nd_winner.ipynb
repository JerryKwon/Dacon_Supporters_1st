{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BCn-Ei0goIto"
   },
   "source": [
    "## Dacon  13회 2019 Jeju BigData Competition-퇴근시간 버스승차인원 예측 모델링 경진대회\n",
    "## Dining (팀명)\n",
    "## 2019년 12월 8일 (제출날짜)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BX-vpeTqq1kK"
   },
   "source": [
    "#### path\n",
    "```\n",
    "├── data\n",
    "│   ├── train.csv\n",
    "│   ├── test.csv\n",
    "│   └── submission.csv\n",
    "├── scripts\n",
    "│   └── scripts.ipynb\n",
    "└── predict\n",
    "     └── lgb_best.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "14QfsTDzoItr"
   },
   "source": [
    "## 1. 라이브러리 및 데이터\n",
    "## Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T03:49:42.549127Z",
     "start_time": "2019-12-10T03:49:42.545127Z"
    }
   },
   "outputs": [],
   "source": [
    "#-*- coding:utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T04:00:49.149156Z",
     "start_time": "2019-12-10T04:00:49.140181Z"
    }
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T04:51:55.532856Z",
     "start_time": "2019-12-10T04:51:35.891414Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "wRb21FeBoIts"
   },
   "outputs": [],
   "source": [
    "# Library 예시\n",
    "import os\n",
    "\n",
    "# handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import gc\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# prevent overfit\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# model\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "SEED=42\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.round(np.sqrt(mean_squared_error(y_true, y_pred)), 5)\n",
    "\n",
    "########################### BASIC SETTING\n",
    "os.chdir('./data')\n",
    "# os.chdir('../data')\n",
    "seed_everything(SEED)\n",
    "TARGET = '18~20_ride'\n",
    "\n",
    "########################### DATA LOAD\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "sub = pd.read_csv('submission_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ww8mr2r_oItw"
   },
   "source": [
    "## 1. 데이터 전처리\n",
    "## Data Cleansing & Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TZlWkNrdoItw"
   },
   "outputs": [],
   "source": [
    "def df_copy(tr_df, te_df):\n",
    "    tr = tr_df.copy();te = te_df.copy()\n",
    "    return tr, te\n",
    "\n",
    "def base_preprocessing(tr_df, te_df):\n",
    "    tr, te = df_copy(tr_df, te_df)\n",
    "\n",
    "    tr['bus_route_id'] = tr['bus_route_id'].apply(lambda x: str(x)[:-4]).astype(int)\n",
    "    te['bus_route_id'] = te['bus_route_id'].apply(lambda x: str(x)[:-4]).astype(int)\n",
    "    tr['station_name2'] = tr['station_name'].apply(lambda x: str(x)[:2])\n",
    "    te['station_name2'] = te['station_name'].apply(lambda x: str(x)[:2])\n",
    "    tr['station_name'] = tr['station_name'].apply(lambda x: x.replace(' ', ''))\n",
    "    te['station_name'] = te['station_name'].apply(lambda x: x.replace(' ', ''))\n",
    "\n",
    "    le = LabelEncoder().fit(pd.concat([tr['station_name'], te['station_name']]))\n",
    "    le2 = LabelEncoder().fit(pd.concat([tr['station_name2'], te['station_name2']]))\n",
    "    for df in [tr, te]:\n",
    "        df['day'] = pd.to_datetime(df['date']).dt.day\n",
    "        df['week'] = pd.to_datetime(df['date']).dt.week\n",
    "        df['weekday'] = pd.to_datetime(df['date']).dt.weekday\n",
    "        df['station_name'] = le.transform(df['station_name'])\n",
    "        df['station_name2'] = le2.transform(df['station_name2'])\n",
    "\n",
    "        df['6~8_ride'] = df[['6~7_ride','7~8_ride']].sum(1)\n",
    "        df['6~9_ride'] = df[['6~7_ride','7~8_ride','8~9_ride']].sum(1)\n",
    "        df['6~10_ride'] = df[['6~7_ride','7~8_ride','8~9_ride', '9~10_ride']].sum(1)\n",
    "        df['6~8_takeoff'] = df[['6~7_takeoff','7~8_takeoff']].sum(1)\n",
    "        df['6~9_takeoff'] = df[['6~7_takeoff','7~8_takeoff','8~9_takeoff']].sum(1)\n",
    "        df['6~10_takeoff'] = df[['6~7_takeoff','7~8_takeoff','8~9_takeoff', '9~10_takeoff']].sum(1)\n",
    "    te['day'] = te['day']+30\n",
    "    return tr, te\n",
    "\n",
    "def lat_long_create(tr_df, te_df):\n",
    "    tr, te = df_copy(tr_df, te_df)\n",
    "    tr['lat_long'] = np.round(tr['latitude'], 2).astype(str) + np.round(tr['longitude'], 2).astype(str)\n",
    "    te['lat_long'] = np.round(te['latitude'], 2).astype(str) + np.round(te['longitude'], 2).astype(str)\n",
    "    le = LabelEncoder().fit(pd.concat([tr['lat_long'], te['lat_long']]))\n",
    "    tr['station_lat_long'] = le.transform(tr['lat_long'])\n",
    "    te['station_lat_long'] = le.transform(te['lat_long'])\n",
    "\n",
    "    tr['lat_long'] = np.round(tr['latitude'], 3).astype(str) + np.round(tr['longitude'], 2).astype(str)\n",
    "    te['lat_long'] = np.round(te['latitude'], 3).astype(str) + np.round(te['longitude'], 2).astype(str)\n",
    "    le = LabelEncoder().fit(pd.concat([tr['lat_long'], te['lat_long']]))\n",
    "    tr['station_lat_long2'] = le.transform(tr['lat_long'])\n",
    "    te['station_lat_long2'] = le.transform(te['lat_long'])\n",
    "    return tr, te\n",
    "\n",
    "def feature_combine(tr_df, te_df):\n",
    "    tr, te = df_copy(tr_df, te_df)\n",
    "    for df in [tr, te]:\n",
    "        df['bus_route_id_station_code'] = ((df['bus_route_id']).astype(str) + (df['station_code']).astype(str)).astype('category')\n",
    "        df['bus_route_id_station_lat_long'] = ((df['bus_route_id']).astype(str) + (df['station_lat_long']).astype(str)).astype('category')\n",
    "    return tr, te\n",
    "\n",
    "def category_transform(tr_df, te_df, columns):\n",
    "    tr, te = df_copy(tr_df, te_df)\n",
    "    for df in [tr, te]:\n",
    "        df[columns] = df[columns].astype(str).astype('category')\n",
    "    return tr, te\n",
    "\n",
    "def frequency_encoding(tr_df, te_df, columns, normalize=False):\n",
    "    tr, te = df_copy(tr_df, te_df)\n",
    "    for col in columns:\n",
    "        if not normalize:\n",
    "            freq_encode = pd.concat([tr[col], te[col]]).value_counts()\n",
    "            tr[col+'_fq_enc'] = tr[col].map(freq_encode)\n",
    "            te[col+'_fq_enc'] = te[col].map(freq_encode)\n",
    "        else:\n",
    "            freq_encode = pd.concat([tr[col], te[col]]).value_counts(normalize=True)\n",
    "            tr[col+'_fq_enc_nor'] = tr[col].map(freq_encode)\n",
    "            te[col+'_fq_enc_nor'] = te[col].map(freq_encode)\n",
    "    return tr, te\n",
    "\n",
    "def remove_outlier(tr_df, te_df, columns):\n",
    "    tr, te = df_copy(tr_df, te_df)\n",
    "    for col in columns:\n",
    "        tr[col] = np.where(tr[col].isin(te[col]), tr[col], 0)\n",
    "        te[col] = np.where(te[col].isin(tr[col]), te[col], 0)\n",
    "    return tr, te\n",
    "\n",
    "def day_agg(tr_df, te_df, merge_columns, columns, aggs=['mean']):\n",
    "    tr, te = df_copy(tr_df, te_df)\n",
    "    for merge_column in merge_columns:\n",
    "        for col in columns:\n",
    "            for agg in aggs:\n",
    "                valid = pd.concat([tr[[merge_column, col]], te[[merge_column, col]]])\n",
    "                new_cn = merge_column + '_' + agg + '_' + col\n",
    "                if agg=='quantile':\n",
    "                    valid = valid.groupby(merge_column)[col].quantile(0.8).reset_index().rename(columns={col:new_cn})\n",
    "                else:\n",
    "                    valid = valid.groupby(merge_column)[col].agg([agg]).reset_index().rename(columns={agg:new_cn})\n",
    "                valid.index = valid[merge_column].tolist()\n",
    "                valid = valid[new_cn].to_dict()\n",
    "            \n",
    "                tr[new_cn] = tr[merge_column].map(valid)\n",
    "                te[new_cn] = te[merge_column].map(valid)\n",
    "    return tr, te\n",
    "\n",
    "def sub_day_agg(tr_df, te_df, merge_columns, date_columns, columns, aggs=['mean']):\n",
    "    tr, te = df_copy(tr_df, te_df)\n",
    "    for merge_column in merge_columns:\n",
    "        for date in date_columns:\n",
    "            tr['mc_date'] = tr[merge_column].astype(str) + '_' +tr[date].astype(str)\n",
    "            te['mc_date'] = te[merge_column].astype(str) + '_' +te[date].astype(str)\n",
    "            for col in columns:\n",
    "                for agg in aggs:\n",
    "                    valid = pd.concat([tr[['mc_date', col]], te[['mc_date', col]]])\n",
    "                    new_cn = merge_column + '_' + date + '_' + col + '_' + agg\n",
    "                    if agg=='quantile':\n",
    "                        valid = valid.groupby('mc_date')[col].quantile(0.8).reset_index().rename(columns={col:new_cn})\n",
    "                    else:\n",
    "                        valid = valid.groupby('mc_date')[col].agg([agg]).reset_index().rename(columns={agg:new_cn})\n",
    "                    valid.index = valid['mc_date'].tolist()\n",
    "                    valid = valid[new_cn].to_dict()\n",
    "                \n",
    "                    tr[new_cn] = tr['mc_date'].map(valid)\n",
    "                    te[new_cn] = te['mc_date'].map(valid)\n",
    "    tr = tr.drop(columns=['mc_date'])\n",
    "    te = te.drop(columns=['mc_date'])\n",
    "    return tr, te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L5pQmzbLqVVI"
   },
   "outputs": [],
   "source": [
    "########################### Final features list\n",
    "remove_features = ['id', 'date', 'in_out', TARGET]\n",
    "ride_take = ['6~7_ride', '7~8_ride', '8~9_ride', '9~10_ride', '10~11_ride', '11~12_ride', '6~7_takeoff', '7~8_takeoff', '8~9_takeoff', '9~10_takeoff', '10~11_takeoff', '11~12_takeoff']\n",
    "\n",
    "remove_features += ['day', 'week', 'weekday', 'lat_long']\n",
    "tr, te = base_preprocessing(train_df, test_df)\n",
    "tr, te = lat_long_create(tr, te)\n",
    "tr, te = feature_combine(tr, te)\n",
    "\n",
    "ride_take += ['6~8_ride', '6~9_ride', '6~10_ride', '6~8_takeoff', '6~9_takeoff', '6~10_takeoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-2QOG5DqkXy"
   },
   "outputs": [],
   "source": [
    "tr, te = day_agg(tr, te, merge_columns=['day'], columns=ride_take, aggs=['mean'])\n",
    "tr, te = sub_day_agg(tr, te, merge_columns=['bus_route_id', 'station_code', 'station_lat_long'], date_columns=['day'], columns=ride_take, aggs=['mean'])\n",
    "tr, te = sub_day_agg(tr, te, merge_columns=['bus_route_id', 'station_code', 'station_name', 'station_lat_long'], date_columns=['day'], columns=ride_take, aggs=['quantile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmxhHZ5Cqleq"
   },
   "outputs": [],
   "source": [
    "category_features = ['bus_route_id', 'station_code', 'station_name', 'station_name2', 'station_lat_long', 'station_lat_long2', 'bus_route_id_station_code', 'bus_route_id_station_lat_long']\n",
    "tr, te = frequency_encoding(tr, te, category_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3d8gCzydoIt0"
   },
   "source": [
    "## 2. 변수 선택 및 모델 구축 및 학습\n",
    "## Feature Engineering & Initial Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fKqsi7-cqn4a"
   },
   "outputs": [],
   "source": [
    "########################### Model\n",
    "def make_predictions(model, tr_df, tt_df, features_columns, target, params, category_feature=[''], NFOLDS=4, oof_save=False, clip=999, SEED=SEED):\n",
    "    X,y = tr_df[features_columns], tr_df[target]\n",
    "    kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    oof = np.zeros(len(tr_df))\n",
    "    pred = np.zeros(len(tt_df))\n",
    "    fi_df = pd.DataFrame()\n",
    "    \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        print('Fold:',fold_)\n",
    "        tr_data = lgb.Dataset(X.loc[trn_idx], label=y[trn_idx].clip(0, clip))\n",
    "        vl_data = lgb.Dataset(X.loc[val_idx], label=y[val_idx])\n",
    "        if model=='lgb':\n",
    "            estimator = lgb.train(params, tr_data, valid_sets = [tr_data, vl_data], verbose_eval = 500)\n",
    "            fi_df = pd.concat([fi_df, pd.DataFrame(sorted(zip(estimator.feature_importance(), features_columns)), columns=['Value', 'Feature'])])\n",
    "        \n",
    "        oof[val_idx] = estimator.predict(X.loc[val_idx])\n",
    "        pred += estimator.predict(tt_df[features_columns])/NFOLDS\n",
    "        del estimator\n",
    "        gc.collect()\n",
    "\n",
    "    oof = np.where(oof>0, oof, 0)\n",
    "    pred = np.where(pred>0, pred, 0)\n",
    "\n",
    "    if oof_save:\n",
    "        if model=='lgb':\n",
    "            np.save('/content/oof_lgb.npy', oof)\n",
    "            np.save('/content/pred_lgb.npy', pred)\n",
    "        elif model=='cat':\n",
    "            np.save('/content/oof_cat.npy', oof)\n",
    "            np.save('/content/pred_cat.npy', pred)\n",
    "\n",
    "    tt_df[target] = pred\n",
    "    print('OOF RMSE:', rmse(y, oof))\n",
    "    \n",
    "    try:\n",
    "        fi_df = fi_df.groupby('Feature').mean().reset_index().sort_values('Value')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return tt_df[['id', target]], fi_df\n",
    "## -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TFHpZmNWoIt1"
   },
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "        'objective':'regression',\n",
    "        'boosting_type':'gbdt',\n",
    "        'metric':'rmse',\n",
    "        'n_jobs':-1,\n",
    "        'learning_rate':0.003,\n",
    "        'num_leaves': 700,\n",
    "        'max_depth':-1,\n",
    "        'min_child_weight':5,\n",
    "        'colsample_bytree': 0.3,\n",
    "        'subsample':0.7,\n",
    "        'n_estimators':50000,\n",
    "        'gamma':0,\n",
    "        'reg_lambda':0.05,\n",
    "        'reg_alpha':0.05,\n",
    "        'verbose':-1,\n",
    "        'seed': SEED,\n",
    "        'early_stopping_rounds':50\n",
    "    }\n",
    "tr, te = remove_outlier(tr, te, category_features)\n",
    "tr, te = category_transform(tr, te, category_features)\n",
    "\n",
    "features_columns = [col for col in tr.columns if col not in remove_features]\n",
    "test_predictions, fi = make_predictions('lgb', tr, te, features_columns, TARGET, lgb_params, category_feature=category_features, NFOLDS=5, oof_save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4KRm429-oIt6"
   },
   "source": [
    "## 3. 결과\n",
    "## Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpAxN-whoIt7"
   },
   "outputs": [],
   "source": [
    "test_predictions.to_csv('../predict/lgb_model.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "모델링 대회 제출 파일 양식.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
