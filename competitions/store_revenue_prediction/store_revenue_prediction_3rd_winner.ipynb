{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Dacon-9회-펀다-상점-매출-예측-경진대회-모델링-경진대회\" data-toc-modified-id=\"Dacon-9회-펀다-상점-매출-예측-경진대회-모델링-경진대회-0.1\">Dacon 9회 펀다 상점 매출 예측 경진대회 모델링 경진대회</a></span></li><li><span><a href=\"#vvmatorin-(팀명)\" data-toc-modified-id=\"vvmatorin-(팀명)-0.2\">vvmatorin (팀명)</a></span></li><li><span><a href=\"#2019년-08월-29일-(제출날짜)\" data-toc-modified-id=\"2019년-08월-29일-(제출날짜)-0.3\">2019년 08월 29일 (제출날짜)</a></span></li></ul></li><li><span><a href=\"#모델링-코드-작성방법\" data-toc-modified-id=\"모델링-코드-작성방법-1\">모델링 코드 작성방법</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.-라이브러리-및-데이터\" data-toc-modified-id=\"1.-라이브러리-및-데이터-1.1\">1. 라이브러리 및 데이터</a></span></li><li><span><a href=\"#Library-&amp;-Data\" data-toc-modified-id=\"Library-&amp;-Data-1.2\">Library &amp; Data</a></span></li><li><span><a href=\"#2.-데이터-전처리\" data-toc-modified-id=\"2.-데이터-전처리-1.3\">2. 데이터 전처리</a></span></li><li><span><a href=\"#Data-Cleansing-&amp;-Pre-Processing\" data-toc-modified-id=\"Data-Cleansing-&amp;-Pre-Processing-1.4\">Data Cleansing &amp; Pre-Processing</a></span></li><li><span><a href=\"#3.-탐색적-자료분석\" data-toc-modified-id=\"3.-탐색적-자료분석-1.5\">3. 탐색적 자료분석</a></span></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-1.6\">Exploratory Data Analysis</a></span></li><li><span><a href=\"#4.-변수-선택-및-모델-구축\" data-toc-modified-id=\"4.-변수-선택-및-모델-구축-1.7\">4. 변수 선택 및 모델 구축</a></span></li><li><span><a href=\"#Feature-Engineering-&amp;-Initial-Modeling\" data-toc-modified-id=\"Feature-Engineering-&amp;-Initial-Modeling-1.8\">Feature Engineering &amp; Initial Modeling</a></span></li><li><span><a href=\"#5.-모델-학습-및-검증\" data-toc-modified-id=\"5.-모델-학습-및-검증-1.9\">5. 모델 학습 및 검증</a></span></li><li><span><a href=\"#Model-Tuning-&amp;-Evaluation\" data-toc-modified-id=\"Model-Tuning-&amp;-Evaluation-1.10\">Model Tuning &amp; Evaluation</a></span></li><li><span><a href=\"#6.-결과-및-결언\" data-toc-modified-id=\"6.-결과-및-결언-1.11\">6. 결과 및 결언</a></span></li><li><span><a href=\"#Conclusion-&amp;-Discussion\" data-toc-modified-id=\"Conclusion-&amp;-Discussion-1.12\">Conclusion &amp; Discussion</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Dacon 9회 펀다 상점 매출 예측 경진대회 모델링 경진대회\n",
    "## vvmatorin (팀명)\n",
    "## 2019년 08월 29일 (제출날짜)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# 모델링 코드 작성방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 1. 라이브러리 및 데이터\n",
    "## Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T09:40:22.713570Z",
     "start_time": "2019-09-10T09:40:22.709582Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T09:40:25.157035Z",
     "start_time": "2019-09-10T09:40:22.714569Z"
    },
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import shap\n",
    "\n",
    "import lightgbm as lgbm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from workalendar.asia import SouthKorea\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T09:40:34.405292Z",
     "start_time": "2019-09-10T09:40:25.158056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>store_id</th>\n",
       "      <th>type_of_business</th>\n",
       "      <th>card_id</th>\n",
       "      <th>card_company</th>\n",
       "      <th>transacted_date</th>\n",
       "      <th>transacted_time</th>\n",
       "      <th>amount</th>\n",
       "      <th>installment_term</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>기타 미용업</td>\n",
       "      <td>0</td>\n",
       "      <td>b</td>\n",
       "      <td>2016-06-01</td>\n",
       "      <td>13:13</td>\n",
       "      <td>1857.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-01 13:13:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>기타 미용업</td>\n",
       "      <td>1</td>\n",
       "      <td>h</td>\n",
       "      <td>2016-06-01</td>\n",
       "      <td>18:12</td>\n",
       "      <td>857.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-01 18:12:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>기타 미용업</td>\n",
       "      <td>2</td>\n",
       "      <td>c</td>\n",
       "      <td>2016-06-01</td>\n",
       "      <td>18:52</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-01 18:52:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>기타 미용업</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>2016-06-01</td>\n",
       "      <td>20:22</td>\n",
       "      <td>7857.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-01 20:22:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>기타 미용업</td>\n",
       "      <td>4</td>\n",
       "      <td>c</td>\n",
       "      <td>2016-06-02</td>\n",
       "      <td>11:06</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-02 11:06:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  region  store_id type_of_business  card_id card_company transacted_date  \\\n",
       "0    NaN         0           기타 미용업        0            b      2016-06-01   \n",
       "1    NaN         0           기타 미용업        1            h      2016-06-01   \n",
       "2    NaN         0           기타 미용업        2            c      2016-06-01   \n",
       "3    NaN         0           기타 미용업        3            a      2016-06-01   \n",
       "4    NaN         0           기타 미용업        4            c      2016-06-02   \n",
       "\n",
       "  transacted_time       amount  installment_term                date  \n",
       "0           13:13  1857.142857                 0 2016-06-01 13:13:00  \n",
       "1           18:12   857.142857                 0 2016-06-01 18:12:00  \n",
       "2           18:52  2000.000000                 0 2016-06-01 18:52:00  \n",
       "3           20:22  7857.142857                 0 2016-06-01 20:22:00  \n",
       "4           11:06  2000.000000                 0 2016-06-02 11:06:00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data.\n",
    "\n",
    "data = pd.read_csv('../../DATA/funda_train.csv')\n",
    "\n",
    "data['date'] = data['transacted_date'] + ' ' + data['transacted_time']\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 2. 데이터 전처리\n",
    "## Data Cleansing & Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:22.870Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing negative values.\n",
    "\n",
    "def remove_negative(data):\n",
    "    data_pos = data[data.amount > 0]\n",
    "    data_neg = data[data.amount < 0]\n",
    "    drop_ind = set()\n",
    "    \n",
    "    for neg in data_neg.itertuples():\n",
    "        amount = abs(neg.amount)\n",
    "        \n",
    "        row = data_pos[data_pos.store_id == neg.store_id]\n",
    "        row = row[row.card_id == neg.card_id]\n",
    "        row = row[row.card_company == neg.card_company]\n",
    "        row = row[row.amount >= amount]\n",
    "        row = row[row.date <= neg.date]\n",
    "        row = row[~row.index.isin(drop_ind)]\n",
    "        \n",
    "        if len(row[row.amount == amount]) > 0:\n",
    "            row = row[row.amount == amount]\n",
    "            matched_row = row[row.date == max(row.date)]\n",
    "            drop_ind.update(matched_row.index)\n",
    "        elif len(row[row.amount > amount]) > 0:\n",
    "            matched_row = row[row.date == max(row.date)]\n",
    "            data_pos.loc[matched_row.index, 'amount'] += neg.amount\n",
    "            \n",
    "    data_pos.drop(drop_ind, axis = 0, inplace = True)\n",
    "    print('Count - with negative: {}'.format(len(data)))\n",
    "    print('Count - without negative: {}'.format(len(data_pos)))\n",
    "\n",
    "    return data_pos\n",
    "    \n",
    "data = remove_negative(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:22.872Z"
    },
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Removing unnecessary columns.\n",
    "\n",
    "drop_cols = ['installment_term', 'transacted_date', 'transacted_time']\n",
    "data.drop(drop_cols, axis = 1, inplace = True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:22.873Z"
    }
   },
   "outputs": [],
   "source": [
    "# Business-related features expansion.\n",
    "\n",
    "# I grouped different bussiness in smaller categories and organized them into a separate .CSV\n",
    "\n",
    "biz_info = pd.read_csv('../data/business_info.csv')\n",
    "data = data.merge(biz_info, on = 'type_of_business', how = 'left')\n",
    "\n",
    "# Numerating strings as categories.\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "text_cols = data.dtypes.pipe(lambda x: x[x == 'object']).index.tolist()\n",
    "data[text_cols] = data[text_cols].apply(lambda x: encoder.fit_transform(x.tolist()))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:22.875Z"
    }
   },
   "outputs": [],
   "source": [
    "# Date-related features extraction and preprocessing.\n",
    "\n",
    "cal = SouthKorea()\n",
    "\n",
    "data['weekday'] = data['date'].dt.weekday\n",
    "data['holiday'] = data['date'].apply(lambda x: cal.is_holiday(x)).astype(int)\n",
    "data['workday'] = data['date'].apply(lambda x: cal.is_working_day(x)).astype(int)\n",
    "\n",
    "data.set_index('date', drop = False, inplace = True)\n",
    "data.sort_index(inplace = True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:22.878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Active stores.\n",
    "\n",
    "active_stores = data.last('3M')['store_id'].unique()\n",
    "data['is_active'] = data['store_id'].isin(active_stores).astype(int)\n",
    "\n",
    "# Global aggregates by store.\n",
    "\n",
    "transforms = ['sum', 'mean', 'median', 'size', 'std', 'min', 'max']\n",
    "group = data.groupby('store_id')\n",
    "\n",
    "for t in transforms: \n",
    "    data[t] = group['amount'].transform(t)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:22.880Z"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot encoding.\n",
    "\n",
    "dummy_cols = ['card_company', 'weekday']\n",
    "\n",
    "for col in dummy_cols:\n",
    "    data = pd.concat([data, pd.get_dummies(data[col], prefix = col)], axis=1)\n",
    "\n",
    "data.drop(dummy_cols, axis = 1, inplace = True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 3. 탐색적 자료분석\n",
    "## Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.099Z"
    },
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Adding moving averages and last-N stats for 3 months groups.\n",
    "\n",
    "data['group_id'] = data.groupby(pd.Grouper(level = 0, freq = '1QS-DEC')).ngroup()\n",
    "\n",
    "days = [1, 7, 14, 28, 46]\n",
    "sid_list = data['store_id'].unique()\n",
    "gid_list = data['group_id'].unique()\n",
    "\n",
    "sales = data.groupby(['store_id', 'group_id', pd.Grouper(level = 0, freq = '1D')])[['amount']].sum()\n",
    "features = []\n",
    "\n",
    "for sid in sid_list:\n",
    "    for gid in gid_list:\n",
    "        try:\n",
    "            store = sales.xs(sid, level = 0).xs(gid, level = 0)\n",
    "        except Exception as error:\n",
    "            continue\n",
    "            \n",
    "        dictionary = {'store_id': sid, 'group_id': gid}\n",
    "        \n",
    "        # Calculating decay rates.\n",
    "        \n",
    "        for d in days:\n",
    "            amount = store['amount'].resample('{}D'.format(d)).sum(min_count = 1).dropna()\n",
    "            date_dif = amount.reset_index()['date'].diff(-1).dt.days.abs()\n",
    "            \n",
    "            amount_dif = (amount.shift(-1) / amount).reset_index(drop = True)\n",
    "            dictionary['decay{}_mean'.format(d)] = amount_dif.mean()\n",
    "            dictionary['decay{}_mean~time'.format(d)] = (amount_dif / date_dif).mean()\n",
    "            \n",
    "            amount_dif = np.log1p(amount_dif)\n",
    "            dictionary['decay{}_log_mean'.format(d)] = (amount_dif / date_dif).mean()\n",
    "            dictionary['decay{}_log_mean~time'.format(d)] = (amount_dif / date_dif).mean()\n",
    "            \n",
    "        for d in days: \n",
    "            store['ma{}'.format(d)] = store['amount'].rolling('{}D'.format(d)).mean()\n",
    "            \n",
    "        for d1 in days:\n",
    "            l = store.last('{}D'.format(d1))\n",
    "            \n",
    "            dictionary['last{}_sum'.format(d1)] = l.amount.sum()\n",
    "            dictionary['last{}_mean'.format(d1)] = l.amount.mean()\n",
    "            dictionary['last{}_median'.format(d1)] = l.amount.median()\n",
    "            \n",
    "            for d2 in days:\n",
    "                dictionary['ma{}_last{}_mean'.format(d2, d1)] = l['ma{}'.format(d2)].mean()\n",
    "                \n",
    "        features.append(dictionary)\n",
    "            \n",
    "features = pd.DataFrame(features)\n",
    "\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Appending new features.\n",
    "\n",
    "ndata = data.merge(features, on = ['group_id', 'store_id'], how = 'left')\n",
    "ndata.set_index('date', drop = False, inplace = True)\n",
    "ndata.sort_index(inplace = True)\n",
    "\n",
    "# Aggregating data by 3 months duration.\n",
    "\n",
    "aggregations = {\n",
    "    'amount': {\n",
    "        'sales_current': 'sum',\n",
    "        'sales_mean': 'mean',\n",
    "        'sales_median': 'median',\n",
    "        'sales_count': 'count',\n",
    "        'sales_std': 'std',\n",
    "        'sales_mad': 'mad',\n",
    "        'sales_min': 'min',\n",
    "        'sales_max': 'max'\n",
    "    },\n",
    "    'holiday': {\n",
    "        'holidays_%': 'mean',\n",
    "        'holidays_count': 'sum'\n",
    "    },\n",
    "    'workday': {\n",
    "        'workdays_%': 'mean',\n",
    "        'workdays_count': 'sum'\n",
    "    },\n",
    "    'card_id': {\n",
    "        'customers_unique': 'nunique',\n",
    "    },\n",
    "    'date': {\n",
    "        'year_min': lambda x: min(x).year,\n",
    "        'year_max': lambda x: max(x).year,\n",
    "        'doty_min': lambda x: min(x).dayofyear,\n",
    "        'doty_max': lambda x: max(x).dayofyear,\n",
    "        'time_span': lambda x: (max(x) - min(x)).days + 1\n",
    "    }\n",
    "}\n",
    "\n",
    "dummy_cols_agg = {col: {col: 'mean'} for col in ndata.columns if col.startswith(tuple(dummy_cols))}\n",
    "aggregations.update(dummy_cols_agg)\n",
    "\n",
    "col_regex = '|'.join(transforms)\n",
    "first_cols = ['region', 'type_of_business', 'business_area', 'kind_of_service', 'is_active']\n",
    "first_cols.extend(ndata.columns[ndata.columns.str.contains(col_regex, regex = True)])\n",
    "first_cols_agg = {col: {col: 'first'} for col in first_cols}\n",
    "aggregations.update(first_cols_agg)\n",
    "\n",
    "ndata = ndata.groupby(['store_id', pd.Grouper(level = 0, freq = '1QS-DEC')])\n",
    "ndata = ndata.agg(aggregations)\n",
    "ndata.columns = ndata.columns.droplevel(0)\n",
    "\n",
    "# Sales and moving averages lags.\n",
    "\n",
    "gdata = ndata.groupby(level = 0)\n",
    "lag_cols = set(first_cols) - set(transforms) - set(text_cols)\n",
    "\n",
    "for i in range(1, 9):\n",
    "    ndata['sales_diff_{}'.format(i)] = gdata['sales_current'].diff(i)\n",
    "    ndata['sales_last_{}'.format(i)] = gdata['sales_current'].shift(i)\n",
    "    ndata['sales_pctd_{}'.format(i)] = gdata['sales_current'].pct_change(i)\n",
    "    \n",
    "    ndata['sales_diff_last{}'.format(i)] = gdata['sales_current'].diff().shift(i)\n",
    "    ndata['sales_pctd_last{}'.format(i)] = gdata['sales_current'].pct_change().shift(i)\n",
    "    \n",
    "    ndata['sales_last_{}_current_%'.format(i)] = ndata['sales_last_{}'.format(i)] / ndata['sales_current']\n",
    "    ndata['sales_current_last_{}_%'.format(i)] = ndata['sales_current'] / ndata['sales_last_{}'.format(i)]\n",
    "    ndata['sales_dcay_{}'.format(i)] = ndata['sales_current'] / ndata['sales_last_{}'.format(i)]\n",
    "    ndata['sales_dcay{}_log'.format(i)] = np.log1p(ndata['sales_dcay_{}'.format(i)])\n",
    "    \n",
    "    for col in lag_cols:\n",
    "        ndata['{}_lag_{}'.format(col, i)] = gdata[col].shift(i)\n",
    "    \n",
    "# Additional features and feature interactions.\n",
    "\n",
    "ndata['sales_mean_%'] = ndata['sales_mean'] / ndata['mean']\n",
    "ndata['sales_range'] = ndata['sales_max'] - ndata['sales_min']\n",
    "ndata['customers_unique_%'] = ndata['customers_unique'] / ndata['sales_count']\n",
    "ndata['sales_diff_1_mean'] = ndata.groupby(level = 0)['sales_diff_1'].transform('mean')\n",
    "ndata['sales_pctd_1_mean'] = ndata.groupby(level = 0)['sales_pctd_1'].transform('mean')\n",
    "ndata['sales_current_last_1_%_mean'] = ndata.groupby(level = 0)['sales_current_last_1_%'].transform('mean')\n",
    "\n",
    "for t in transforms: \n",
    "    ndata['rank_{}'.format(t)] = ndata[t].rank(method = 'dense', ascending = False)   \n",
    "    \n",
    "for d in days:\n",
    "    ndata['last{}_sum_current_%'.format(d)] = ndata['last{}_sum'.format(d)] / ndata['sales_current']    \n",
    "    \n",
    "ndata['range'] = ndata['max'] - ndata['min']\n",
    "ndata['rank_range'] = ndata['range'].rank(method = 'dense', ascending = False)\n",
    "ndata['sales_current_rank_sum_%'] = ndata['sales_current'] / ndata['rank_sum']\n",
    "\n",
    "for i in range(2, 9):\n",
    "    ndata['sales_diff_{}_diff_1_mean_%'.format(i)] = ndata['sales_diff_{}'.format(i)] / ndata['sales_diff_1_mean']\n",
    "    ndata['sales_pctd_{}_pctd_1_mean_%'.format(i)] = ndata['sales_pctd_{}'.format(i)] / ndata['sales_pctd_1_mean']\n",
    "    ndata['sales_pctd_{}_diff_1_mean_%'.format(i)] = ndata['sales_pctd_{}'.format(i)] / ndata['sales_diff_1_mean']\n",
    "    ndata['sales_diff_{}_pctd_1_mean_%'.format(i)] = ndata['sales_diff_{}'.format(i)] / ndata['sales_pctd_1_mean']\n",
    "\n",
    "    \n",
    "# Post-processing.\n",
    "    \n",
    "ndata.drop(['sum', 'size'], axis = 1, inplace = True)\n",
    "ndata.reset_index(inplace = True)    \n",
    "ndata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.103Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adding truncated SVD features.\n",
    "\n",
    "imp = SimpleImputer(strategy = \"constant\", fill_value = 0)\n",
    "tr_svd = TruncatedSVD(n_components = 20)\n",
    "ex_cols = ['date', 'store_id', 'sales_current', 'sales_current_last_1_%', 'is_active',\n",
    "           'region', 'type_of_business', 'business_area', 'kind_of_service']\n",
    "\n",
    "x = ndata.loc[:, ~ndata.columns.isin(ex_cols)]\n",
    "scaled_x = StandardScaler().fit_transform(x.values)\n",
    "imputed_x = imp.fit_transform(scaled_x)\n",
    "truncated_x = tr_svd.fit_transform(imputed_x)\n",
    "\n",
    "scaled_x = pd.DataFrame(scaled_x, columns = x.columns)\n",
    "truncated_x = pd.DataFrame(truncated_x)\n",
    "\n",
    "tdata = pd.concat([ndata[ex_cols], scaled_x, truncated_x], axis = 1)\n",
    "tdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 4. 변수 선택 및 모델 구축\n",
    "## Feature Engineering & Initial Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.254Z"
    },
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Preparing training and validation datasets.\n",
    "\n",
    "def split_data(data, tr_multiplier = 1, is_expanding = False, verbose = True):\n",
    "    n = len(data.date.unique()) - (tr_multiplier + 1)\n",
    "    group = list(data.groupby('date'))\n",
    "    group = [g[-1] for g in group]\n",
    "    splits = []\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        ceil = i + tr_multiplier\n",
    "        floor = 0 if is_expanding else i\n",
    "\n",
    "        train = pd.concat(group[floor : ceil])\n",
    "        train = train.sort_values('date').groupby('store_id').tail(1)\n",
    "        valid = group[ceil: ceil + 1][0]\n",
    "        y_val = []\n",
    "        \n",
    "        if verbose:\n",
    "            print('Fold {}: TRAIN - {} to {}'.format(i, train.date.min(), valid.date.min()))\n",
    "\n",
    "        for j in range(0, 2):\n",
    "            y = group[ceil + j : ceil + j + 1][0][['store_id', 'sales_current_last_1_%']]\n",
    "            y['sales_current_last_1_%'] = np.log1p(np.log1p(y['sales_current_last_1_%']))\n",
    "            y.columns = ['store_id', 'y']\n",
    "            y_val.append(y)\n",
    "            \n",
    "        train = train.merge(y_val[0], on = 'store_id', how = 'inner')\n",
    "        valid = valid.merge(y_val[-1], on = 'store_id', how = 'inner')\n",
    "\n",
    "        train = train.drop('date', axis = 1).reset_index(drop = True)\n",
    "        valid = valid.drop('date', axis = 1).reset_index(drop = True)\n",
    "        \n",
    "        splits.append((train, valid))\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "splits = split_data(tdata, 1, True)\n",
    "splits[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.256Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fitting model and cross validation.\n",
    "\n",
    "metric = 'l1'\n",
    "cat_features = ['region', 'type_of_business', 'business_area', 'kind_of_service']\n",
    "\n",
    "def train_lgbm(splits, metric = 'l1', categories = '',\n",
    "               max_depth = 6, num_leaves = 48, feature_fraction = 0.8,\n",
    "               num_iterations = 3000, early_stopping = 100):\n",
    "    models = []\n",
    "    params = {\n",
    "        'learning_rate': 0.01,\n",
    "        'boosting': 'gbdt',\n",
    "        'objective': 'regression_l1',\n",
    "        'num_leaves': num_leaves,\n",
    "        'max_bin': 255,\n",
    "        'max_depth': max_depth,\n",
    "        'metric': metric,\n",
    "        'num_iterations': num_iterations,\n",
    "        'early_stopping': early_stopping,\n",
    "        'cat_smooth': 10,\n",
    "        'feature_fraction': feature_fraction\n",
    "    }\n",
    "\n",
    "    for (t, v) in splits:\n",
    "        d_train = lgbm.Dataset(t.drop('y', axis = 1), label = t['y'])\n",
    "        d_valid = lgbm.Dataset(v.drop('y', axis = 1), label = v['y'], reference = d_train)\n",
    "\n",
    "        evals_result = {}\n",
    "        clf = lgbm.train(params, d_train, valid_sets = [d_train, d_valid], \n",
    "                         evals_result = evals_result, verbose_eval = 1500,\n",
    "                         categorical_feature = categories)\n",
    "\n",
    "        model = {\n",
    "            'score': list(clf.best_score.values()),\n",
    "            'evals': evals_result,\n",
    "            'model': clf,\n",
    "            'best': clf.best_iteration\n",
    "        }\n",
    "\n",
    "        models.append((model, t, v))\n",
    "        \n",
    "    return models\n",
    "\n",
    "                         \n",
    "models = train_lgbm(splits, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.258Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = [m['score'] for (m, _, _) in models]\n",
    "scores = [{'train': score[0][metric], 'valid': score[-1][metric]} for score in scores]\n",
    "scores = pd.DataFrame(scores)\n",
    "scores.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 5. 모델 학습 및 검증\n",
    "## Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.403Z"
    },
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "for (m, _, _) in models:\n",
    "    lgbm.plot_importance(m['model'], importance_type = 'gain', max_num_features = 15, figsize = (15, 6))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.404Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for (m, _, _) in models:\n",
    "    lgbm.plot_metric(m['evals'], metric = metric, figsize = (15, 6))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.406Z"
    }
   },
   "outputs": [],
   "source": [
    "# Making and plotting predictions for stores, which are common across all folds.\n",
    "\n",
    "sid_list = set()\n",
    "\n",
    "for (_, t, v) in models:    \n",
    "    ids = set(t.store_id) & set(v.store_id)\n",
    "    sid_list = sid_list & ids if bool(sid_list) else ids\n",
    "    \n",
    "for (m, _, v) in models:    \n",
    "    d_plot = v[v['store_id'].isin(sid_list)]\n",
    "    d_plot['y_pred'] = m['model'].predict(d_plot, num_iteration = m['best'])\n",
    "    d_plot = np.expm1(np.expm1(d_plot.set_index('store_id')[['y', 'y_pred']]))\n",
    "    d_plot['error'] = d_plot['y'] - d_plot['y_pred']\n",
    "    d_plot = d_plot.groupby('store_id')['error'].mean()\n",
    "    d_plot.sort_values(inplace = True)\n",
    "\n",
    "    d_plot.iloc[::20].plot.bar(figsize = (15, 6), title = 'Prediction error')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.408Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exploring feature interactions with SHAP.\n",
    "\n",
    "def plot_feature_interaction(data, interaction_values): \n",
    "    img = np.abs(interaction_values).sum(0)\n",
    "\n",
    "    for i in range(img.shape[0]):\n",
    "        img[i,i] = 0\n",
    "\n",
    "    inds = np.argsort(-img.sum(0))[:30]\n",
    "    img = img[inds,:][:,inds]\n",
    "\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.yticks(range(img.shape[0]), data.columns[inds], rotation = 0, horizontalalignment = \"right\")\n",
    "    plt.xticks(range(img.shape[0]), data.columns[inds], rotation = 50, horizontalalignment = \"left\")\n",
    "    plt.gca().xaxis.tick_top()\n",
    "    plt.show()\n",
    "\n",
    "for (m, _, v) in models:\n",
    "    valid = v.drop('y', axis = 1)\n",
    "    explainer = shap.TreeExplainer(m['model'])\n",
    "    shap_interaction_values = explainer.shap_interaction_values(valid)\n",
    "    plot_feature_interaction(valid, shap_interaction_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.410Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adding error-based features.\n",
    "\n",
    "def add_error_features(data, models):\n",
    "    errors = pd.DataFrame() \n",
    "    \n",
    "    for (m, t, v) in models:    \n",
    "        valid = v[['store_id', 'y']]\n",
    "        train = t[['store_id', 'y']]\n",
    "\n",
    "        valid['y_pred'] = m['model'].predict(v, num_iteration = m['best'])\n",
    "        train['y_pred'] = m['model'].predict(t, num_iteration = m['best'])\n",
    "\n",
    "        train = np.expm1(np.expm1(train.set_index('store_id')[['y', 'y_pred']])).reset_index()\n",
    "        valid = np.expm1(np.expm1(valid.set_index('store_id')[['y', 'y_pred']])).reset_index()\n",
    "\n",
    "        train['error'] = train['y'] - train['y_pred']\n",
    "        valid['error'] = valid['y'] - valid['y_pred']\n",
    "\n",
    "        merged = train.merge(valid, on = 'store_id', how = 'outer')[['store_id', 'error_x', 'error_y']]\n",
    "        errors = pd.concat([errors, merged], axis = 0)\n",
    "\n",
    "    errors = errors.groupby('store_id')[['error_x', 'error_y']].mean().reset_index()\n",
    "    errors['error_x_abs'], errors['error_y_abs'] = errors['error_x'].abs(), errors['error_y'].abs()\n",
    "    errors['error_y_x_%'] = errors['error_y'] / errors['error_x']\n",
    "\n",
    "    errors['error_abs_sum'] = errors['error_x_abs'] + errors['error_y_abs']\n",
    "    errors['error_abs_mean'] = 0.5 * errors['error_abs_sum'] \n",
    "    errors['error_sum'] = errors['error_x'] + errors['error_y']\n",
    "    errors['error_mean'] = 0.5 * errors['error_sum']\n",
    "\n",
    "    quantiles = [0, .01, .05, .2, .5, .8, .95, .99, 1]\n",
    "\n",
    "    error_cols = [col for col in errors.columns if col.startswith('error')]\n",
    "    for col in error_cols:\n",
    "        errors['{}_group'.format(col)] = pd.qcut(errors[col], quantiles, labels = False)\n",
    "        errors['rank_{}'.format(col)] = errors[col].rank(method = 'dense')\n",
    "\n",
    "    erdata = data.merge(errors, on = 'store_id', how = 'left')\n",
    "    \n",
    "    return erdata\n",
    "\n",
    "    \n",
    "erdata = add_error_features(tdata, models)\n",
    "erdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.412Z"
    }
   },
   "outputs": [],
   "source": [
    "# We retrain the model with error-based features.\n",
    "\n",
    "ersplits = split_data(erdata, 1, True, False)\n",
    "ermodels = train_lgbm(ersplits, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.414Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = [m['score'] for (m, _, _) in ermodels]\n",
    "scores = [{'train': score[0][metric], 'valid': score[-1][metric]} for score in scores]\n",
    "scores = pd.DataFrame(scores)\n",
    "scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.416Z"
    }
   },
   "outputs": [],
   "source": [
    "# We remove all of the features, which are not giving enough gains.\n",
    "\n",
    "important_features = set()\n",
    "\n",
    "for (m, _, v) in ermodels:\n",
    "    valid = v.drop('y', axis = 1)\n",
    "    explainer = shap.TreeExplainer(m['model'])\n",
    "    shap_values = explainer.shap_values(valid)\n",
    "    shap_sum = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    importance_df = pd.DataFrame([valid.columns.tolist(), shap_sum.tolist()]).T\n",
    "    importance_df.columns = ['feature', 'shap_importance']\n",
    "    important_features |= set(importance_df[importance_df['shap_importance'] > 10 ** -3]['feature'])\n",
    "      \n",
    "len(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.418Z"
    }
   },
   "outputs": [],
   "source": [
    "idata = erdata[important_features | set(['date', 'store_id', 'sales_current', 'sales_current_last_1_%', 'is_active'])]\n",
    "isplits = split_data(idata, 1, True, False)\n",
    "imodels = train_lgbm(isplits, metric, max_depth = 5, num_leaves = 32, \n",
    "                     feature_fraction = 0.9, num_iterations = 2000, early_stopping = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.477Z"
    }
   },
   "outputs": [],
   "source": [
    "# We reorganize features by adding features interactions for most imporant features and then\n",
    "# factorizing the matrix by tr-svd.\n",
    "\n",
    "poly = PolynomialFeatures(2, include_bias = False, interaction_only = True)\n",
    "\n",
    "z = idata.loc[:, ~idata.columns.isin(ex_cols)]\n",
    "imputed_z = imp.fit_transform(z.values)\n",
    "poly_z = poly.fit_transform(imputed_z)\n",
    "truncated_z = tr_svd.fit_transform(poly_z)\n",
    "truncated_z = pd.DataFrame(truncated_x)\n",
    "\n",
    "zdata = pd.concat([idata, truncated_z], axis = 1)\n",
    "zdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.479Z"
    }
   },
   "outputs": [],
   "source": [
    "zsplits = split_data(zdata, 1, True, False)\n",
    "zmodels = train_lgbm(zsplits, metric, max_depth = 5, num_leaves = 32, \n",
    "                     feature_fraction = 0.9, num_iterations = 2000, early_stopping = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 6. 결과 및 결언\n",
    "## Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.632Z"
    },
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Getting submission data.\n",
    "\n",
    "def get_submission(sbm_data, models, average_out = False):    \n",
    "    test = sbm_data.sort_values('date').groupby('store_id').tail(1).drop('date', axis = 1)\n",
    "    test.sort_values('store_id', inplace = True)\n",
    "\n",
    "    m = models[-1][0]\n",
    "    prediction = m['model'].predict(test, num_iteration = m['best'])\n",
    "    prediction = np.expm1(np.expm1(prediction))\n",
    "\n",
    "    submission = test[['store_id', 'sales_current', 'is_active']]\n",
    "    submission['amount'] = submission['sales_current'] * prediction\n",
    "    submission.set_index('store_id', inplace = True)\n",
    "    print(submission['amount'].mean())\n",
    "    \n",
    "    if average_out:\n",
    "        submission['amount'] = 0.5 * submission['amount'] + 0.5 * submission['sales_current']\n",
    "        print(submission['amount'].mean())\n",
    "        \n",
    "    submission.loc[submission['is_active'] == 0, 'amount'] = 0\n",
    "    submission = submission[['amount']]\n",
    "    print(submission['amount'].mean())\n",
    "    \n",
    "    return submission\n",
    "    \n",
    "\n",
    "submission = get_submission(zdata, zmodels, True)    \n",
    "# submission.to_csv('../notes/submissions/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T09:40:23.633Z"
    }
   },
   "outputs": [],
   "source": [
    "# Final thoughts:\n",
    "#\n",
    "# This model due to the nature of preprocessing suffers from leaks, but I didn't have\n",
    "# enough time to get through reengineering process.\n",
    "# It would have probably been a good idea to keep information about installments\n",
    "# and use it as an estimation for level of luxury for different shops.\n",
    "# I didn't work with N/A values in my solution and just left them as a separate category,\n",
    "# but playing around with using KNN to fill them might have given better results.\n",
    "# The worst part about this model is that it uses post-processing by averaging out results\n",
    "# of the last 3 months. \n",
    "# Another way to improve the model would have been to train many Lasso-regression models\n",
    "# separately for each store and use their predictions as features for LGBM model.\n",
    "# Working with lower granularity of data might have worked as well.\n",
    "#\n",
    "# Thank you!"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
